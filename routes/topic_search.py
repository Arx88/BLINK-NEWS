# routes/topic_search.py

from flask import Blueprint, jsonify, request
import os
import json
from datetime import datetime
import threading
import time
import hashlib
from models.topic_searcher import TopicSearcher # ADD THIS
from models.superior_note_generator import SuperiorNoteGenerator # ADD THIS

topic_search_bp = Blueprint('topic_search', __name__)
active_searches = {}

# --- HELPER FUNCTIONS ---
def _get_results_filepath(search_key):
    """Construye la ruta al archivo de resultados para una b√∫squeda."""
    results_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'topic_searches')
    return os.path.join(results_dir, f"search_{search_key}.json")

def _save_search_results(search_key, results):
    """Guarda los resultados de una b√∫squeda en un archivo JSON."""
    filepath = _get_results_filepath(search_key)
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"Resultados de b√∫squeda guardados: {filepath}")
    except Exception as e:
        print(f"Error guardando resultados de b√∫squeda: {e}")

def _get_search_results(search_key):
    """Obtiene los resultados de una b√∫squeda desde un archivo JSON."""
    filepath = _get_results_filepath(search_key)
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error obteniendo resultados de b√∫squeda: {e}")
    return None

def _delete_search_results(search_key):
    """Elimina un archivo de resultados de b√∫squeda antiguo si existe."""
    filepath = _get_results_filepath(search_key)
    if os.path.exists(filepath):
        os.remove(filepath)
        print(f"Archivo de resultados antiguo eliminado: {filepath}")

# --- API ROUTES ---
@topic_search_bp.route('/search-topic', methods=['POST'])
def search_topic():
    """Inicia una nueva b√∫squeda por tema, limpiando cualquier resultado anterior."""
    data = request.get_json()
    if not data or 'topic' not in data or not data['topic'].strip():
        return jsonify({'error': 'Se requiere especificar un tema'}), 400

    topic = data['topic'].strip()
    hours_back = data.get('hours_back', 24)
    search_key = f"{topic.lower().replace(' ', '_')}_{hours_back}"

    if search_key in active_searches:
        return jsonify({
            'status': 'processing',
            'message': f'B√∫squeda ya en progreso para "{topic}". Por favor espere...',
            'topic': topic
        })

    # Borrar resultados viejos antes de empezar
    _delete_search_results(search_key)

    active_searches[search_key] = {
        'topic': topic, 'started_at': datetime.now().isoformat(), 'status': 'starting'
    }

    threading.Thread(
        target=process_topic_search,
        args=(topic, hours_back, data.get('max_sources', 5), search_key),
        daemon=True
    ).start()

    return jsonify({
        'status': 'started',
        'message': f'Iniciando b√∫squeda para "{topic}". Esto puede tomar unos minutos...',
        'topic': topic,
        'search_key': search_key
    })

@topic_search_bp.route('/search-status/<search_key>', methods=['GET'])
def get_search_status(search_key):
    """Obtiene el estado de una b√∫squeda. L√≥gica corregida para perfecta sincronizaci√≥n."""
    if search_key in active_searches:
        # La tarea est√° activa. Devolver su estado actual.
        return jsonify(active_searches[search_key])
    else:
        # La tarea NO est√° activa. Significa que ha terminado o nunca existi√≥.
        # Ahora s√≠, buscamos el archivo de resultados.
        results = _get_search_results(search_key)
        if results:
            # Se encontr√≥ un archivo, la tarea est√° completada.
            return jsonify({'status': 'completed', 'results': results})
        else:
            # No est√° activa y no hay archivo. La b√∫squeda no se encuentra.
            return jsonify({'status': 'not_found', 'message': 'B√∫squeda no encontrada o expirada.'}), 404

# --- BACKGROUND PROCESS ---
def process_topic_search(topic, hours_back, max_sources, search_key):
    """
    Procesa la b√∫squeda en segundo plano utilizando TopicSearcher y SuperiorNoteGenerator,
    y limpia el estado al finalizar.
    """
    try:
        # 1. Instanciar los modelos correctos
        searcher = TopicSearcher()
        note_generator = SuperiorNoteGenerator()

        # 2. Actualizar estado e iniciar la b√∫squeda de noticias agrupadas
        print(f"üîé Iniciando b√∫squeda y agrupaci√≥n de noticias para: '{topic}'")
        active_searches[search_key]['status'] = 'searching_news'

        # El m√©todo search_topic_news ya busca y agrupa las noticias por evento
        grouped_news = searcher.search_topic_news(
            topic=topic,
            hours_back=hours_back,
            max_sources=max_sources
        )

        if not grouped_news:
            print(f"üü° No se encontraron grupos de noticias para '{topic}'.")
            results = {
                'status': 'no_results',
                'message': 'No se encontraron suficientes noticias de m√∫ltiples fuentes sobre este tema en el per√≠odo de tiempo seleccionado.',
                'topic': topic,
                'superior_notes': [],
                'total_groups_found': 0,
                'notes_generated': 0,
                'timestamp': datetime.now().isoformat()
            }
            _save_search_results(search_key, results)
            return # Termina la ejecuci√≥n del hilo

        # 3. Actualizar estado y comenzar la generaci√≥n de notas
        print(f"üß† Encontrados {len(grouped_news)} grupos de noticias. Iniciando generaci√≥n de Notas Superiores.")
        active_searches[search_key]['status'] = 'generating_notes'

        all_superior_notes = []
        for i, group in enumerate(grouped_news):
            print(f"--- Procesando grupo {i+1}/{len(grouped_news)} ---")
            try:
                # 4. Generar una Nota Superior para cada grupo de art√≠culos
                # Este m√©todo ya incluye la creaci√≥n de la nota y el ultra resumen
                superior_note = note_generator.generate_superior_note(group, topic)
                all_superior_notes.append(superior_note)
            except Exception as e:
                print(f"Error generando nota para el grupo {i+1}: {e}")
                continue # Si un grupo falla, contin√∫a con el siguiente

        # 5. Ensamblar y guardar los resultados finales
        print(f"‚úÖ Proceso completado. Se generaron {len(all_superior_notes)} Notas Superiores.")
        final_results = {
            'status': 'success',
            'topic': topic,
            'superior_notes': all_superior_notes,
            'total_groups_found': len(grouped_news),
            'notes_generated': len(all_superior_notes),
            'timestamp': datetime.now().isoformat()
        }
        _save_search_results(search_key, final_results)

    except Exception as e:
        print(f"‚ùå Error cr√≠tico en el proceso de b√∫squeda: {e}")
        error_results = {
            'status': 'error',
            'message': f'Error procesando la b√∫squeda: {str(e)}',
            'topic': topic,
            'timestamp': datetime.now().isoformat()
        }
        _save_search_results(search_key, error_results)
    finally:
        # Limpiar la tarea de la lista de b√∫squedas activas para permitir nuevas b√∫squedas
        if search_key in active_searches:
            del active_searches[search_key]
            print(f"La b√∫squeda '{search_key}' ha sido marcada como finalizada.")
